import re
import spacy
from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity
import numpy as np

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# APP SETUP
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
app = FastAPI()

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

print("ğŸ”„ Loading spaCy model (en_core_web_sm)...")
nlp = spacy.load("en_core_web_sm")
print("âœ… spaCy model loaded.")

print("ğŸ”„ Loading SentenceTransformer model (all-MiniLM-L6-v2)...")
model = SentenceTransformer('all-MiniLM-L6-v2')
print("âœ… SentenceTransformer model loaded.")


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# HELPER: Extract resume sections
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def extract_sections(text):
    """Split resume text into experience, skills, and education sections."""
    sections = {"experience": "", "skills": "", "education": ""}
    lines = text.split('\n')
    current_section = "skills"  # Default bucket

    for line in lines:
        l = line.upper().strip()
        if "EXPERIENCE" in l or "WORK" in l or "PROJECTS" in l:
            current_section = "experience"
        elif "EDUCATION" in l or "COLLEGE" in l or "ACADEMIC" in l:
            current_section = "education"
        elif "SKILLS" in l or "TECHNOLOGIES" in l or "TECH STACK" in l:
            current_section = "skills"
        sections[current_section] += line + " "

    print("\nğŸ“‚ â”€â”€ SECTION EXTRACTION â”€â”€")
    for sec_name, sec_text in sections.items():
        preview = sec_text.strip()[:120] + ("..." if len(sec_text.strip()) > 120 else "")
        print(f"   [{sec_name.upper():>12}] ({len(sec_text.strip()):>5} chars) â†’ {preview}")

    return sections


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# HELPER: Extract keywords using spaCy NLP
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def extract_keywords(text):
    """Extract meaningful keywords (nouns, proper nouns, adjectives) using spaCy."""
    doc = nlp(text.lower())
    keywords = set()
    for token in doc:
        if token.pos_ in ("NOUN", "PROPN", "ADJ") and not token.is_stop and len(token.text) > 2:
            keywords.add(token.text)
    # Also extract named entities
    for ent in doc.ents:
        keywords.add(ent.text.lower())
    return keywords


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# HELPER: Calculate bonus points
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def calculate_bonus(text, resume_sections):
    """Award bonus points for impact metrics, leadership, prestige, and scale."""
    bonus = 0
    text_upper = text.upper()

    # 1. METRIC-BASED IMPACT (percentages, dollar values, action verbs)
    metrics_pattern = r'(\d+%)|(\\$\d+)|(REDUCED|INCREASED|IMPROVED|SAVED|OPTIMIZED|BOOSTED)'
    if re.search(metrics_pattern, text_upper):
        bonus += 5
        print("   ğŸ† +5  Metric-based impact detected")

    # 2. LEADERSHIP & OWNERSHIP SIGNALS
    leadership_keywords = ["LEAD", "MANAGED", "MENTORED", "ARCHITECTED", "OWNED", "INITIATED", "HEADED", "DIRECTED"]
    if any(word in text_upper for word in leadership_keywords):
        bonus += 5
        print("   ğŸ† +5  Leadership signal detected")

    # 3. TOP-TIER INSTITUTIONS (Universities & Companies)
    prestige_keywords = [
        "IIT", "NIT", "BITS", "STANFORD", "MIT", "HARVARD", "OXFORD", "CAMBRIDGE",
        "GOOGLE", "AMAZON", "META", "MICROSOFT", "NETFLIX", "APPLE", "UBER", "STRIPE"
    ]
    combined_text = (resume_sections.get('experience', '') +
                     resume_sections.get('education', '')).upper()
    if any(org in combined_text for org in prestige_keywords):
        bonus += 10
        print("   ğŸ† +10 Prestige institution/company detected")

    # 4. SCALE & COMPLEXITY
    scale_keywords = ["SCALE", "DISTRIBUTED", "MICROSERVICES", "KUBERNETES", "HIGH-TRAFFIC", "LATENCY", "MILLION", "BILLION"]
    if any(word in text_upper for word in scale_keywords):
        bonus += 5
        print("   ğŸ† +5  Scale/complexity signal detected")

    print(f"   â”€â”€ Total Bonus: {bonus} points")
    return bonus


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# HEALTH CHECK
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
@app.get("/health")
async def health_check():
    print("ğŸ’š Health check hit")
    return {"status": "ok"}


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# MAIN ENDPOINT: Calculate Weighted ATS Score
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
@app.post("/calculate_weighted_score")
async def calculate_weighted_score(data: dict):
    print("\n" + "=" * 60)
    print("ğŸ“¥ ATS SCORE REQUEST RECEIVED")
    print("=" * 60)

    jd = data.get("jd", "")
    resume_raw = data.get("resume", "")

    # â”€â”€ Input validation â”€â”€
    if not jd or not jd.strip():
        print("âŒ ERROR: Job description is empty")
        return {"error": "Job description (jd) is required", "ats_score": 0}
    if not resume_raw or not resume_raw.strip():
        print("âŒ ERROR: Resume text is empty")
        return {"error": "Resume text (resume) is required", "ats_score": 0}

    print(f"ğŸ“ JD Preview   : {jd[:100]}...")
    print(f"ğŸ“„ Resume Preview: {resume_raw[:100]}...")

    # â”€â”€ Step 1: Parse Resume into Sections â”€â”€
    print("\nâ”€â”€ STEP 1: Extracting resume sections â”€â”€")
    resume_sections = extract_sections(resume_raw)

    # â”€â”€ Step 2: Generate Embeddings â”€â”€
    print("\nâ”€â”€ STEP 2: Generating embeddings â”€â”€")
    jd_vector = model.encode([jd])
    exp_vector = model.encode([resume_sections['experience']]) if resume_sections['experience'].strip() else model.encode([""])
    skill_vector = model.encode([resume_sections['skills']]) if resume_sections['skills'].strip() else model.encode([""])
    edu_vector = model.encode([resume_sections['education']]) if resume_sections['education'].strip() else model.encode([""])
    print("   âœ… Embeddings generated for JD + 3 resume sections")

    # â”€â”€ Step 3: Calculate Sectional Similarities â”€â”€
    print("\nâ”€â”€ STEP 3: Calculating cosine similarities â”€â”€")
    exp_sim = float(cosine_similarity(jd_vector, exp_vector)[0][0])
    skill_sim = float(cosine_similarity(jd_vector, skill_vector)[0][0])
    edu_sim = float(cosine_similarity(jd_vector, edu_vector)[0][0])
    print(f"   Experience Similarity : {exp_sim:.4f} ({exp_sim * 100:.2f}%)")
    print(f"   Skills Similarity     : {skill_sim:.4f} ({skill_sim * 100:.2f}%)")
    print(f"   Education Similarity  : {edu_sim:.4f} ({edu_sim * 100:.2f}%)")

    # â”€â”€ Step 4: Apply Weights (Experience 60%, Skills 30%, Education 10%) â”€â”€
    print("\nâ”€â”€ STEP 4: Applying weights (60% exp, 30% skills, 10% edu) â”€â”€")
    weighted_score = (exp_sim * 0.60) + (skill_sim * 0.30) + (edu_sim * 0.10)
    base_score = round(weighted_score * 100, 2)
    print(f"   Base Weighted Score: {base_score}%")

    # â”€â”€ Step 5: Calculate Bonus Points â”€â”€
    print("\nâ”€â”€ STEP 5: Calculating bonus points â”€â”€")
    bonus_points = calculate_bonus(resume_sections.get('experience', ''), resume_sections)

    # â”€â”€ Step 6: Compute Final Score (capped at 100) â”€â”€
    final_score = min(100, round(base_score + bonus_points, 2))
    print(f"\nâ”€â”€ STEP 6: Final Score â”€â”€")
    print(f"   Base: {base_score} + Bonus: {bonus_points} = {final_score}% (capped at 100)")

    # â”€â”€ Step 7: Extract & Match Keywords â”€â”€
    print("\nâ”€â”€ STEP 7: Keyword Matching â”€â”€")
    jd_keywords = extract_keywords(jd)
    resume_keywords = extract_keywords(resume_raw)
    matched_keywords = sorted(list(jd_keywords & resume_keywords))
    missing_keywords = sorted(list(jd_keywords - resume_keywords))
    print(f"   JD Keywords     : {len(jd_keywords)} found")
    print(f"   Resume Keywords : {len(resume_keywords)} found")
    print(f"   âœ… Matched       : {len(matched_keywords)} â†’ {matched_keywords[:15]}")
    print(f"   âŒ Missing        : {len(missing_keywords)} â†’ {missing_keywords[:15]}")

    # â”€â”€ Build Response â”€â”€
    response = {
        "ats_score": final_score,
        "breakdown": {
            "experience": round(exp_sim * 100, 2),
            "skills": round(skill_sim * 100, 2),
            "education": round(edu_sim * 100, 2),
            "bonus": bonus_points,
        },
        "keywords": {
            "matched": matched_keywords,
            "missing": missing_keywords[:20],  # Limit to top 20
        }
    }

    print(f"\nğŸ“¤ RESPONSE: {response}")
    print("=" * 60 + "\n")

    return response


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# RUN SERVER
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
if __name__ == "__main__":
    import uvicorn
    print("\nğŸš€ Starting ATS Python Server on http://0.0.0.0:8000")
    uvicorn.run(app, host="0.0.0.0", port=8000)